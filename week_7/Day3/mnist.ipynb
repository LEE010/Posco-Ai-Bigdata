{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SOURCE_URL = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "FILENAME = SOURCE_URL.split('/')[-1]\n",
    "DATA_DIR = './datasets'\n",
    "\n",
    "def maybe_download(data_dir):\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.isfile(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading {} {:.1f} %'.format(\n",
    "                FILENAME, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully donloaded', FILENAME, statinfo.st_size, 'bytes.')\n",
    "\n",
    "def load(data_dir, subset='train'):\n",
    "    maybe_download(data_dir)\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    \n",
    "    f = gzip.open(filepath, 'rb')\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "    f.close()\n",
    "    \n",
    "    if subset == 'train':\n",
    "        trainx, trainy = train_set\n",
    "        trainx = trainx.astype(np.float32).reshape(trainx.shape[0], 28, 28)\n",
    "        trainy = trainy.astype(np.uint8)\n",
    "        return trainx, trainy\n",
    "    elif subset == 'test':\n",
    "        testx, testy = test_set\n",
    "        testx = testx.astype(np.float32).reshape(testx.shape[0], 28, 28)\n",
    "        testy = testy.astype(np.uint8)\n",
    "        return testx, testy\n",
    "    elif subset== 'valid':\n",
    "        validx, validy = valid_set\n",
    "        validx = validx.astype(np.float32).reshape(validx.shape[0], 28, 28)\n",
    "        validy = validy.astype(np.uint8)\n",
    "        return validx, validy\n",
    "    else:\n",
    "        raise NotImplementedError('subset should be train or valid or test')\n",
    "\n",
    "# Load data\n",
    "train_data, train_label = load(DATA_DIR, 'train')\n",
    "valid_data, valid_label = load(DATA_DIR, 'valid')\n",
    "test_data, test_label = load(DATA_DIR, 'test')\n",
    "\n",
    "# concatenate train and valid data as train data\n",
    "train_data = np.concatenate((train_data, valid_data))\n",
    "train_label = np.concatenate((train_label, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sample_data = train_data[4]\n",
    "ax1.imshow(sample_data, cmap=plt.cm.Greys);\n",
    "ax2.hist(sample_data, bins=20, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "batch_size = 100 # 한 에폭마다 셔플 후 128개를(2^n)개 샘플링 해야 정석!\n",
    "num_display = 100\n",
    "\n",
    "def get_model(X,by):\n",
    "    X = tf.expand_dims(X, axis=3) #3차원으로 늘린다 (None,28,28) \n",
    "    \n",
    "    with tf.variable_scope('first',reuse=tf.AUTO_REUSE):\n",
    "        outs = tf.layers.conv2d(X, 128, 3, padding='same') # 일반적인 CNN은 크기를 줄이지 않기 때문에 패딩을 넣는다.\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs,2,2)\n",
    "        \n",
    "    with tf.variable_scope('second',reuse=tf.AUTO_REUSE):\n",
    "        outs = tf.layers.conv2d(outs, 256, 3, padding='same')\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs,2,2)\n",
    "        \n",
    "    with tf.variable_scope('third',reuse=tf.AUTO_REUSE):\n",
    "        outs = tf.layers.conv2d(outs, 64, 3, padding='same')\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs,2,2)\n",
    "    \n",
    "    outs = tf.reshape(outs, (-1, outs.shape[1]*outs.shape[2]*outs.shape[3]))\n",
    "    outs = tf.layers.dense(outs, 128)\n",
    "    outs = tf.nn.relu(outs)\n",
    "    outs = tf.layers.dense(outs, 10)\n",
    "    \n",
    "    one_hot = tf.one_hot(by, 10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outs,labels=one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    preds = tf.cast(tf.argmax(tf.nn.softmax(outs),axis = 1), tf.int32)\n",
    "    \n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(by,preds), tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    return {'loss':loss, 'opt':opt, 'preds':preds, \"acc\":acc, 'init': init}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None,28,28))\n",
    "by = tf.placeholder(tf.int32)\n",
    "\n",
    "model = get_model(X,by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 2.2999 acc 0.1300\n",
      "loss 0.2089 acc 0.9200\n",
      "loss 0.1365 acc 0.9500\n",
      "loss 0.0640 acc 0.9800\n",
      "loss 0.0443 acc 0.9900\n",
      "loss 0.0467 acc 0.9800\n",
      "Current iteration 2\n",
      "loss 0.0335 acc 0.9900\n",
      "loss 0.0496 acc 0.9900\n",
      "loss 0.0491 acc 0.9700\n",
      "loss 0.0729 acc 0.9900\n",
      "loss 0.0290 acc 0.9900\n",
      "loss 0.0078 acc 1.0000\n",
      "Current iteration 3\n",
      "loss 0.0416 acc 0.9900\n",
      "loss 0.0184 acc 0.9900\n",
      "loss 0.0423 acc 0.9900\n",
      "loss 0.0595 acc 0.9900\n",
      "loss 0.0202 acc 0.9900\n",
      "loss 0.0030 acc 1.0000\n",
      "Current iteration 4\n",
      "loss 0.0064 acc 1.0000\n",
      "loss 0.0208 acc 0.9900\n",
      "loss 0.0269 acc 0.9900\n",
      "loss 0.0343 acc 0.9900\n",
      "loss 0.0015 acc 1.0000\n",
      "loss 0.0039 acc 1.0000\n",
      "Current iteration 5\n",
      "loss 0.0072 acc 1.0000\n",
      "loss 0.0064 acc 1.0000\n",
      "loss 0.0177 acc 0.9900\n",
      "loss 0.0384 acc 0.9900\n",
      "loss 0.0034 acc 1.0000\n",
      "loss 0.0004 acc 1.0000\n",
      "Current iteration 6\n",
      "loss 0.0126 acc 1.0000\n",
      "loss 0.0179 acc 0.9900\n",
      "loss 0.0124 acc 1.0000\n",
      "loss 0.0153 acc 1.0000\n",
      "loss 0.0044 acc 1.0000\n",
      "loss 0.0022 acc 1.0000\n",
      "Current iteration 7\n",
      "loss 0.0024 acc 1.0000\n",
      "loss 0.0125 acc 0.9900\n",
      "loss 0.0245 acc 0.9900\n",
      "loss 0.0120 acc 0.9900\n",
      "loss 0.0049 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "Current iteration 8\n",
      "loss 0.0009 acc 1.0000\n",
      "loss 0.0215 acc 0.9900\n",
      "loss 0.0209 acc 0.9900\n",
      "loss 0.0149 acc 1.0000\n",
      "loss 0.0006 acc 1.0000\n",
      "loss 0.0032 acc 1.0000\n",
      "Current iteration 9\n",
      "loss 0.0055 acc 1.0000\n",
      "loss 0.0106 acc 0.9900\n",
      "loss 0.0251 acc 0.9900\n",
      "loss 0.0009 acc 1.0000\n",
      "loss 0.0009 acc 1.0000\n",
      "loss 0.0005 acc 1.0000\n",
      "Current iteration 10\n",
      "loss 0.0016 acc 1.0000\n",
      "loss 0.0093 acc 1.0000\n",
      "loss 0.0027 acc 1.0000\n",
      "loss 0.0018 acc 1.0000\n",
      "loss 0.0006 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "Current iteration 11\n",
      "loss 0.0099 acc 0.9900\n",
      "loss 0.0125 acc 0.9900\n",
      "loss 0.0472 acc 0.9900\n",
      "loss 0.0006 acc 1.0000\n",
      "loss 0.0072 acc 1.0000\n",
      "loss 0.0012 acc 1.0000\n",
      "Current iteration 12\n",
      "loss 0.0045 acc 1.0000\n",
      "loss 0.0030 acc 1.0000\n",
      "loss 0.0095 acc 1.0000\n",
      "loss 0.0472 acc 0.9800\n",
      "loss 0.0002 acc 1.0000\n",
      "loss 0.0063 acc 1.0000\n",
      "Current iteration 13\n",
      "loss 0.0149 acc 0.9900\n",
      "loss 0.0241 acc 0.9900\n",
      "loss 0.0106 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "loss 0.0271 acc 0.9800\n",
      "loss 0.0003 acc 1.0000\n",
      "Current iteration 14\n",
      "loss 0.0006 acc 1.0000\n",
      "loss 0.0151 acc 0.9900\n",
      "loss 0.0013 acc 1.0000\n",
      "loss 0.0018 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0003 acc 1.0000\n",
      "Current iteration 15\n",
      "loss 0.0009 acc 1.0000\n",
      "loss 0.0072 acc 1.0000\n",
      "loss 0.0038 acc 1.0000\n",
      "loss 0.0052 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0003 acc 1.0000\n",
      "Current iteration 16\n",
      "loss 0.0011 acc 1.0000\n",
      "loss 0.0015 acc 1.0000\n",
      "loss 0.0112 acc 0.9900\n",
      "loss 0.0006 acc 1.0000\n",
      "loss 0.0005 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "Current iteration 17\n",
      "loss 0.0023 acc 1.0000\n",
      "loss 0.0012 acc 1.0000\n",
      "loss 0.0027 acc 1.0000\n",
      "loss 0.0019 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "Current iteration 18\n",
      "loss 0.0030 acc 1.0000\n",
      "loss 0.0012 acc 1.0000\n",
      "loss 0.0004 acc 1.0000\n",
      "loss 0.0078 acc 0.9900\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0003 acc 1.0000\n",
      "Current iteration 19\n",
      "loss 0.0062 acc 1.0000\n",
      "loss 0.0041 acc 1.0000\n",
      "loss 0.0012 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 20\n",
      "loss 0.0004 acc 1.0000\n",
      "loss 0.0059 acc 1.0000\n",
      "loss 0.0017 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "Current iteration 21\n",
      "loss 0.0005 acc 1.0000\n",
      "loss 0.0077 acc 0.9900\n",
      "loss 0.0011 acc 1.0000\n",
      "loss 0.0012 acc 1.0000\n",
      "loss 0.0027 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 22\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0126 acc 0.9900\n",
      "loss 0.0136 acc 0.9900\n",
      "loss 0.0491 acc 0.9900\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "Current iteration 23\n",
      "loss 0.0115 acc 0.9900\n",
      "loss 0.0013 acc 1.0000\n",
      "loss 0.0056 acc 1.0000\n",
      "loss 0.0008 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0006 acc 1.0000\n",
      "Current iteration 24\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0003 acc 1.0000\n",
      "loss 0.0004 acc 1.0000\n",
      "loss 0.0004 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0003 acc 1.0000\n",
      "Current iteration 25\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0009 acc 1.0000\n",
      "loss 0.0013 acc 1.0000\n",
      "loss 0.0950 acc 0.9800\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0006 acc 1.0000\n",
      "Current iteration 26\n",
      "loss 0.0016 acc 1.0000\n",
      "loss 0.0007 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 27\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 28\n",
      "loss 0.0012 acc 1.0000\n",
      "loss 0.0013 acc 1.0000\n",
      "loss 0.0005 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 29\n",
      "loss 0.0016 acc 1.0000\n",
      "loss 0.0151 acc 0.9900\n",
      "loss 0.0004 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0004 acc 1.0000\n",
      "Current iteration 30\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0025 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0035 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 31\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 32\n",
      "loss 0.0017 acc 1.0000\n",
      "loss 0.0055 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 33\n",
      "loss 0.0112 acc 0.9900\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0513 acc 0.9900\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 34\n",
      "loss 0.0003 acc 1.0000\n",
      "loss 0.0021 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 35\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0212 acc 0.9900\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 36\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 37\n",
      "loss 0.0015 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 38\n",
      "loss 0.0010 acc 1.0000\n",
      "loss 0.0033 acc 1.0000\n",
      "loss 0.0025 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 39\n",
      "loss 0.0004 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0023 acc 1.0000\n",
      "loss 0.0072 acc 0.9900\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 40\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0001 acc 1.0000\n",
      "loss 0.0009 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 41\n",
      "loss 0.0278 acc 0.9900\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0482 acc 0.9800\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 42\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0002 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 43\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 44\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 45\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 46\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 47\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 48\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 49\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 50\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 51\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 52\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 53\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 54\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 55\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 56\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 57\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 58\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 59\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 60\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 61\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 62\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 63\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 64\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 65\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 66\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 67\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 68\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 69\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 70\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 71\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 72\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 73\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 74\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 75\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 76\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 77\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 78\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 79\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 80\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 81\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 82\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 83\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 84\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 85\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 86\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 87\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 88\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 89\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 90\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 91\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 92\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 93\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 94\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 95\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 96\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 97\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 98\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 99\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "Current iteration 100\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n",
      "loss 0.0000 acc 1.0000\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[10000,128,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node first/conv2d/Conv2D (defined at <ipython-input-5-4a1382ccb27d>:10)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](first/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, first/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node Mean/_27}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_109_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'first/conv2d/Conv2D', defined at:\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-6c8b1b2a71e7>\", line 4, in <module>\n    model = get_model(X,by)\n  File \"<ipython-input-5-4a1382ccb27d>\", line 10, in get_model\n    outs = tf.layers.conv2d(X, 128, 3, padding='same') # 일반적인 CNN은 크기를 줄이지 않기 때문에 패딩을 넣는다.\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\n    return layer.apply(inputs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 374, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 757, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 194, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 868, in __call__\n    return self.conv_op(inp, filter)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\n    return self.call(inp, filter)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 204, in __call__\n    name=self.name)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,128,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node first/conv2d/Conv2D (defined at <ipython-input-5-4a1382ccb27d>:10)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](first/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, first/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node Mean/_27}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_109_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10000,128,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node first/conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](first/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, first/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node Mean/_27}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_109_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bd4343765234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         cur_loss, cur_acc = sess.run([model['loss'], model['acc']],\n\u001b[0;32m---> 21\u001b[0;31m                                       feed_dict={X:test_data, by: test_label})\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test: loss {0:.4f} Test: acc {1:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10000,128,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node first/conv2d/Conv2D (defined at <ipython-input-5-4a1382ccb27d>:10)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](first/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, first/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node Mean/_27}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_109_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'first/conv2d/Conv2D', defined at:\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-6c8b1b2a71e7>\", line 4, in <module>\n    model = get_model(X,by)\n  File \"<ipython-input-5-4a1382ccb27d>\", line 10, in get_model\n    outs = tf.layers.conv2d(X, 128, 3, padding='same') # 일반적인 CNN은 크기를 줄이지 않기 때문에 패딩을 넣는다.\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\n    return layer.apply(inputs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 374, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 757, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 194, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 868, in __call__\n    return self.conv_op(inp, filter)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\n    return self.call(inp, filter)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 204, in __call__\n    name=self.name)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/pirl/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,128,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node first/conv2d/Conv2D (defined at <ipython-input-5-4a1382ccb27d>:10)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](first/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, first/conv2d/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node Mean/_27}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_109_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch +1))\n",
    "        \n",
    "        for ind_ in range(0, int(60000 / batch_size)):\n",
    "            batch_X = train_data[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = train_label[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            \n",
    "            _, cur_loss, cur_acc = sess.run([model['opt'], model['loss'], model['acc']],\n",
    "                                            feed_dict={X:batch_X, by: batch_by})\n",
    "            \n",
    "            if ind_ % num_display == 0:\n",
    "                print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    \n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    \n",
    "    for ind_ in range(0,10):\n",
    "        cur_loss, cur_acc = sess.run([model['loss'], model['acc']],\n",
    "                                      feed_dict={X:test_data, by: test_label})\n",
    "            \n",
    "        print('Test: loss {0:.4f} Test: acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

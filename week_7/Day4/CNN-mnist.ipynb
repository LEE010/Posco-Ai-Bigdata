{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SOURCE_URL = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "FILENAME = SOURCE_URL.split('/')[-1]\n",
    "DATA_DIR = './datasets'\n",
    "\n",
    "def maybe_download(data_dir):\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.isfile(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading {} {:.1f} %'.format(\n",
    "                FILENAME, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully donloaded', FILENAME, statinfo.st_size, 'bytes.')\n",
    "\n",
    "def load(data_dir, subset='train'):\n",
    "    maybe_download(data_dir)\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    \n",
    "    f = gzip.open(filepath, 'rb')\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "    f.close()\n",
    "    \n",
    "    if subset == 'train':\n",
    "        trainx, trainy = train_set\n",
    "        trainx = trainx.astype(np.float32).reshape(trainx.shape[0], 28, 28)\n",
    "        trainy = trainy.astype(np.uint8)\n",
    "        return trainx, trainy\n",
    "    elif subset == 'test':\n",
    "        testx, testy = test_set\n",
    "        testx = testx.astype(np.float32).reshape(testx.shape[0], 28, 28)\n",
    "        testy = testy.astype(np.uint8)\n",
    "        return testx, testy\n",
    "    elif subset== 'valid':\n",
    "        validx, validy = valid_set\n",
    "        validx = validx.astype(np.float32).reshape(validx.shape[0], 28, 28)\n",
    "        validy = validy.astype(np.uint8)\n",
    "        return validx, validy\n",
    "    else:\n",
    "        raise NotImplementedError('subset should be train or valid or test')\n",
    "\n",
    "# Load data\n",
    "train_data, train_label = load(DATA_DIR, 'train')\n",
    "valid_data, valid_label = load(DATA_DIR, 'valid')\n",
    "test_data, test_label = load(DATA_DIR, 'test')\n",
    "\n",
    "# concatenate train and valid data as train data\n",
    "train_data = np.concatenate((train_data, valid_data))\n",
    "train_label = np.concatenate((train_label, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sample_data = train_data[100]\n",
    "ax1.imshow(sample_data, cmap=plt.cm.Greys);\n",
    "ax2.hist(sample_data, bins=20, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(X,y,is_reuse):\n",
    "    outs = tf.layers.conv2d(X, 128, 3, padding='same', name='conv1', reuse=tf.AUTO_REUSE) # (None, 28, 28, 128)\n",
    "    outs = tf.nn.relu(outs)\n",
    "    outs = tf.layers.max_pooling2d(outs, 2, 2) # (None, 14, 14, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "num_display = 100\n",
    "\n",
    "def get_model(X, by, is_reuse):\n",
    "    X = tf.expand_dims(X, axis=3) # (None, 28, 28, 1)\n",
    "    \n",
    "    with tf.variable_scope('first'):\n",
    "        outs = tf.layers.conv2d(X, 128, 3, padding='same', name='conv1', reuse=tf.AUTO_REUSE) # (None, 28, 28, 128)\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs, 2, 2) # (None, 14, 14, 128)\n",
    "    with tf.variable_scope('second'):\n",
    "        outs = tf.layers.conv2d(outs, 256, 3, padding='same', name='conv2', reuse=is_reuse)\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs, 2, 2) # (None, 7, 7, 256)\n",
    "    with tf.variable_scope('third'):\n",
    "        outs = tf.layers.conv2d(outs, 64, 3, padding='same', name='conv3', reuse=is_reuse)\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs, 2, 2) # (None, 3, 3, 64)\n",
    "    \n",
    "    outs = tf.reshape(outs, (-1, outs.shape[1]*outs.shape[2]*outs.shape[3]))\n",
    "    \n",
    "    with tf.variable_scope('dense'):\n",
    "        outs = tf.layers.dense(outs,128, name='dense1',reuse=is_reuse)\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.dense(outs, 10, name='dense2',reuse=is_reuse)\n",
    "    \n",
    "    one_hot = tf.one_hot(by, 10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outs, \n",
    "                                                      labels=one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    preds = tf.cast(tf.argmax(tf.nn.softmax(outs), axis=1), tf.int32)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(by, preds), tf.float32))\n",
    "    saver = tf.train.Saver(\n",
    "            tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,'second')\n",
    "            +tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,'thrid')\n",
    "            +tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,'dense')\n",
    "    )\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'opt': opt,\n",
    "        'preds': preds,\n",
    "        'acc': acc,\n",
    "        'init': init,\n",
    "        'saver': saver\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 28, 28))\n",
    "by = tf.placeholder(tf.int32)\n",
    "\n",
    "model = get_model(X, by, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 2.3048 acc 0.0800\n",
      "loss 2.3000 acc 0.1000\n",
      "loss 2.2987 acc 0.0900\n",
      "loss 2.2873 acc 0.1700\n",
      "loss 2.2818 acc 0.1700\n",
      "loss 2.2766 acc 0.2600\n",
      "Current iteration 2\n",
      "loss 2.2674 acc 0.2800\n",
      "loss 2.2690 acc 0.2900\n",
      "loss 2.2689 acc 0.2800\n",
      "loss 2.2596 acc 0.3100\n",
      "loss 2.2452 acc 0.3800\n",
      "loss 2.2420 acc 0.4200\n",
      "Current iteration 3\n",
      "loss 2.2260 acc 0.4100\n",
      "loss 2.2278 acc 0.3400\n",
      "loss 2.2300 acc 0.4000\n",
      "loss 2.2143 acc 0.3600\n",
      "loss 2.1857 acc 0.4500\n",
      "loss 2.1842 acc 0.4900\n",
      "Current iteration 4\n",
      "loss 2.1510 acc 0.5200\n",
      "loss 2.1488 acc 0.4600\n",
      "loss 2.1502 acc 0.5400\n",
      "loss 2.1198 acc 0.4900\n",
      "loss 2.0574 acc 0.5700\n",
      "loss 2.0561 acc 0.6300\n",
      "Current iteration 5\n",
      "loss 1.9752 acc 0.7100\n",
      "loss 1.9523 acc 0.6300\n",
      "loss 1.9327 acc 0.6600\n",
      "loss 1.8626 acc 0.6600\n",
      "loss 1.6939 acc 0.7100\n",
      "loss 1.6851 acc 0.6800\n",
      "Current iteration 6\n",
      "loss 1.4681 acc 0.8000\n",
      "loss 1.3719 acc 0.7000\n",
      "loss 1.3432 acc 0.6900\n",
      "loss 1.2067 acc 0.8000\n",
      "loss 0.9589 acc 0.7800\n",
      "loss 1.0052 acc 0.7700\n",
      "Current iteration 7\n",
      "loss 0.7773 acc 0.8300\n",
      "loss 0.7413 acc 0.8400\n",
      "loss 0.8278 acc 0.8100\n",
      "loss 0.6931 acc 0.8200\n",
      "loss 0.5479 acc 0.8700\n",
      "loss 0.6437 acc 0.8500\n",
      "Current iteration 8\n",
      "loss 0.5120 acc 0.8800\n",
      "loss 0.5220 acc 0.8700\n",
      "loss 0.6070 acc 0.8400\n",
      "loss 0.4673 acc 0.8800\n",
      "loss 0.3864 acc 0.9100\n",
      "loss 0.4791 acc 0.8600\n",
      "Current iteration 9\n",
      "loss 0.4011 acc 0.9100\n",
      "loss 0.4361 acc 0.8900\n",
      "loss 0.4996 acc 0.8700\n",
      "loss 0.3535 acc 0.9000\n",
      "loss 0.3060 acc 0.9000\n",
      "loss 0.3863 acc 0.8900\n",
      "Current iteration 10\n",
      "loss 0.3381 acc 0.9300\n",
      "loss 0.3899 acc 0.8900\n",
      "loss 0.4385 acc 0.8800\n",
      "loss 0.2881 acc 0.9300\n",
      "loss 0.2576 acc 0.9100\n",
      "loss 0.3243 acc 0.9100\n",
      "Current iteration 11\n",
      "loss 0.2961 acc 0.9400\n",
      "loss 0.3571 acc 0.9000\n",
      "loss 0.3953 acc 0.8700\n",
      "loss 0.2459 acc 0.9300\n",
      "loss 0.2253 acc 0.9200\n",
      "loss 0.2793 acc 0.9100\n",
      "Current iteration 12\n",
      "loss 0.2658 acc 0.9400\n",
      "loss 0.3329 acc 0.9100\n",
      "loss 0.3600 acc 0.8900\n",
      "loss 0.2174 acc 0.9300\n",
      "loss 0.2010 acc 0.9400\n",
      "loss 0.2424 acc 0.9300\n",
      "Current iteration 13\n",
      "loss 0.2396 acc 0.9400\n",
      "loss 0.3141 acc 0.9000\n",
      "loss 0.3323 acc 0.9100\n",
      "loss 0.1972 acc 0.9500\n",
      "loss 0.1815 acc 0.9400\n",
      "loss 0.2135 acc 0.9400\n",
      "Current iteration 14\n",
      "loss 0.2205 acc 0.9500\n",
      "loss 0.2973 acc 0.9000\n",
      "loss 0.3103 acc 0.9100\n",
      "loss 0.1823 acc 0.9500\n",
      "loss 0.1659 acc 0.9500\n",
      "loss 0.1898 acc 0.9600\n",
      "Current iteration 15\n",
      "loss 0.2052 acc 0.9600\n",
      "loss 0.2825 acc 0.9100\n",
      "loss 0.2918 acc 0.9100\n",
      "loss 0.1711 acc 0.9500\n",
      "loss 0.1529 acc 0.9700\n",
      "loss 0.1703 acc 0.9700\n",
      "Current iteration 16\n",
      "loss 0.1931 acc 0.9600\n",
      "loss 0.2685 acc 0.9100\n",
      "loss 0.2759 acc 0.9300\n",
      "loss 0.1619 acc 0.9500\n",
      "loss 0.1422 acc 0.9700\n",
      "loss 0.1543 acc 0.9700\n",
      "Current iteration 17\n",
      "loss 0.1827 acc 0.9700\n",
      "loss 0.2559 acc 0.9100\n",
      "loss 0.2613 acc 0.9300\n",
      "loss 0.1546 acc 0.9500\n",
      "loss 0.1332 acc 0.9700\n",
      "loss 0.1412 acc 0.9700\n",
      "Current iteration 18\n",
      "loss 0.1743 acc 0.9700\n",
      "loss 0.2434 acc 0.9100\n",
      "loss 0.2489 acc 0.9300\n",
      "loss 0.1490 acc 0.9500\n",
      "loss 0.1254 acc 0.9700\n",
      "loss 0.1300 acc 0.9700\n",
      "Current iteration 19\n",
      "loss 0.1667 acc 0.9700\n",
      "loss 0.2324 acc 0.9100\n",
      "loss 0.2372 acc 0.9300\n",
      "loss 0.1442 acc 0.9500\n",
      "loss 0.1187 acc 0.9800\n",
      "loss 0.1202 acc 0.9700\n",
      "Current iteration 20\n",
      "loss 0.1602 acc 0.9700\n",
      "loss 0.2219 acc 0.9200\n",
      "loss 0.2256 acc 0.9200\n",
      "loss 0.1403 acc 0.9500\n",
      "loss 0.1125 acc 0.9800\n",
      "loss 0.1112 acc 0.9700\n",
      "Current iteration 21\n",
      "loss 0.1542 acc 0.9700\n",
      "loss 0.2119 acc 0.9200\n",
      "loss 0.2155 acc 0.9200\n",
      "loss 0.1375 acc 0.9500\n",
      "loss 0.1069 acc 0.9800\n",
      "loss 0.1034 acc 0.9700\n",
      "Current iteration 22\n",
      "loss 0.1487 acc 0.9700\n",
      "loss 0.2028 acc 0.9300\n",
      "loss 0.2061 acc 0.9200\n",
      "loss 0.1349 acc 0.9500\n",
      "loss 0.1019 acc 0.9800\n",
      "loss 0.0964 acc 0.9700\n",
      "Current iteration 23\n",
      "loss 0.1444 acc 0.9700\n",
      "loss 0.1949 acc 0.9300\n",
      "loss 0.1982 acc 0.9200\n",
      "loss 0.1328 acc 0.9500\n",
      "loss 0.0970 acc 0.9800\n",
      "loss 0.0904 acc 0.9700\n",
      "Current iteration 24\n",
      "loss 0.1405 acc 0.9700\n",
      "loss 0.1875 acc 0.9300\n",
      "loss 0.1907 acc 0.9200\n",
      "loss 0.1310 acc 0.9500\n",
      "loss 0.0928 acc 0.9800\n",
      "loss 0.0849 acc 0.9700\n",
      "Current iteration 25\n",
      "loss 0.1367 acc 0.9700\n",
      "loss 0.1807 acc 0.9300\n",
      "loss 0.1838 acc 0.9200\n",
      "loss 0.1295 acc 0.9500\n",
      "loss 0.0893 acc 0.9800\n",
      "loss 0.0798 acc 0.9700\n",
      "Current iteration 26\n",
      "loss 0.1330 acc 0.9800\n",
      "loss 0.1741 acc 0.9500\n",
      "loss 0.1774 acc 0.9200\n",
      "loss 0.1284 acc 0.9500\n",
      "loss 0.0861 acc 0.9800\n",
      "loss 0.0752 acc 0.9800\n",
      "Current iteration 27\n",
      "loss 0.1296 acc 0.9800\n",
      "loss 0.1682 acc 0.9600\n",
      "loss 0.1717 acc 0.9200\n",
      "loss 0.1273 acc 0.9500\n",
      "loss 0.0833 acc 0.9800\n",
      "loss 0.0712 acc 0.9800\n",
      "Current iteration 28\n",
      "loss 0.1266 acc 0.9800\n",
      "loss 0.1623 acc 0.9600\n",
      "loss 0.1659 acc 0.9300\n",
      "loss 0.1263 acc 0.9500\n",
      "loss 0.0807 acc 0.9800\n",
      "loss 0.0674 acc 0.9800\n",
      "Current iteration 29\n",
      "loss 0.1230 acc 0.9800\n",
      "loss 0.1565 acc 0.9600\n",
      "loss 0.1614 acc 0.9300\n",
      "loss 0.1252 acc 0.9500\n",
      "loss 0.0778 acc 0.9800\n",
      "loss 0.0640 acc 0.9800\n",
      "Current iteration 30\n",
      "loss 0.1197 acc 0.9800\n",
      "loss 0.1504 acc 0.9600\n",
      "loss 0.1560 acc 0.9300\n",
      "loss 0.1233 acc 0.9600\n",
      "loss 0.0750 acc 0.9800\n",
      "loss 0.0611 acc 0.9800\n",
      "Current iteration 31\n",
      "loss 0.1163 acc 0.9800\n",
      "loss 0.1450 acc 0.9600\n",
      "loss 0.1506 acc 0.9300\n",
      "loss 0.1219 acc 0.9600\n",
      "loss 0.0728 acc 0.9800\n",
      "loss 0.0588 acc 0.9800\n",
      "Current iteration 32\n",
      "loss 0.1135 acc 0.9800\n",
      "loss 0.1402 acc 0.9600\n",
      "loss 0.1461 acc 0.9300\n",
      "loss 0.1202 acc 0.9600\n",
      "loss 0.0707 acc 0.9800\n",
      "loss 0.0565 acc 0.9800\n",
      "Current iteration 33\n",
      "loss 0.1112 acc 0.9800\n",
      "loss 0.1357 acc 0.9600\n",
      "loss 0.1420 acc 0.9400\n",
      "loss 0.1198 acc 0.9600\n",
      "loss 0.0691 acc 0.9800\n",
      "loss 0.0545 acc 0.9800\n",
      "Current iteration 34\n",
      "loss 0.1090 acc 0.9800\n",
      "loss 0.1316 acc 0.9600\n",
      "loss 0.1380 acc 0.9400\n",
      "loss 0.1195 acc 0.9600\n",
      "loss 0.0675 acc 0.9800\n",
      "loss 0.0524 acc 0.9800\n",
      "Current iteration 35\n",
      "loss 0.1070 acc 0.9800\n",
      "loss 0.1277 acc 0.9600\n",
      "loss 0.1343 acc 0.9500\n",
      "loss 0.1191 acc 0.9600\n",
      "loss 0.0662 acc 0.9800\n",
      "loss 0.0506 acc 0.9800\n",
      "Current iteration 36\n",
      "loss 0.1051 acc 0.9800\n",
      "loss 0.1236 acc 0.9600\n",
      "loss 0.1307 acc 0.9500\n",
      "loss 0.1186 acc 0.9600\n",
      "loss 0.0651 acc 0.9800\n",
      "loss 0.0490 acc 0.9800\n",
      "Current iteration 37\n",
      "loss 0.1033 acc 0.9800\n",
      "loss 0.1197 acc 0.9600\n",
      "loss 0.1277 acc 0.9600\n",
      "loss 0.1182 acc 0.9600\n",
      "loss 0.0635 acc 0.9800\n",
      "loss 0.0474 acc 0.9800\n",
      "Current iteration 38\n",
      "loss 0.1012 acc 0.9800\n",
      "loss 0.1159 acc 0.9600\n",
      "loss 0.1245 acc 0.9600\n",
      "loss 0.1181 acc 0.9600\n",
      "loss 0.0621 acc 0.9800\n",
      "loss 0.0460 acc 0.9800\n",
      "Current iteration 39\n",
      "loss 0.0996 acc 0.9800\n",
      "loss 0.1123 acc 0.9600\n",
      "loss 0.1210 acc 0.9600\n",
      "loss 0.1178 acc 0.9600\n",
      "loss 0.0607 acc 0.9800\n",
      "loss 0.0449 acc 0.9800\n",
      "Current iteration 40\n",
      "loss 0.0980 acc 0.9800\n",
      "loss 0.1088 acc 0.9600\n",
      "loss 0.1179 acc 0.9600\n",
      "loss 0.1177 acc 0.9600\n",
      "loss 0.0594 acc 0.9800\n",
      "loss 0.0439 acc 0.9900\n",
      "Current iteration 41\n",
      "loss 0.0965 acc 0.9800\n",
      "loss 0.1056 acc 0.9600\n",
      "loss 0.1149 acc 0.9600\n",
      "loss 0.1175 acc 0.9600\n",
      "loss 0.0582 acc 0.9800\n",
      "loss 0.0431 acc 0.9900\n",
      "Current iteration 42\n",
      "loss 0.0954 acc 0.9800\n",
      "loss 0.1024 acc 0.9600\n",
      "loss 0.1122 acc 0.9600\n",
      "loss 0.1174 acc 0.9600\n",
      "loss 0.0570 acc 0.9800\n",
      "loss 0.0425 acc 0.9900\n",
      "Current iteration 43\n",
      "loss 0.0942 acc 0.9800\n",
      "loss 0.0996 acc 0.9600\n",
      "loss 0.1099 acc 0.9600\n",
      "loss 0.1171 acc 0.9600\n",
      "loss 0.0558 acc 0.9800\n",
      "loss 0.0416 acc 0.9900\n",
      "Current iteration 44\n",
      "loss 0.0927 acc 0.9800\n",
      "loss 0.0967 acc 0.9700\n",
      "loss 0.1081 acc 0.9600\n",
      "loss 0.1170 acc 0.9600\n",
      "loss 0.0545 acc 0.9800\n",
      "loss 0.0408 acc 0.9900\n",
      "Current iteration 45\n",
      "loss 0.0915 acc 0.9800\n",
      "loss 0.0941 acc 0.9800\n",
      "loss 0.1061 acc 0.9600\n",
      "loss 0.1169 acc 0.9600\n",
      "loss 0.0535 acc 0.9800\n",
      "loss 0.0399 acc 0.9900\n",
      "Current iteration 46\n",
      "loss 0.0904 acc 0.9800\n",
      "loss 0.0914 acc 0.9800\n",
      "loss 0.1044 acc 0.9600\n",
      "loss 0.1167 acc 0.9600\n",
      "loss 0.0524 acc 0.9800\n",
      "loss 0.0390 acc 0.9900\n",
      "Current iteration 47\n",
      "loss 0.0893 acc 0.9800\n",
      "loss 0.0890 acc 0.9800\n",
      "loss 0.1029 acc 0.9600\n",
      "loss 0.1165 acc 0.9600\n",
      "loss 0.0513 acc 0.9800\n",
      "loss 0.0386 acc 0.9900\n",
      "Current iteration 48\n",
      "loss 0.0885 acc 0.9800\n",
      "loss 0.0868 acc 0.9800\n",
      "loss 0.1015 acc 0.9700\n",
      "loss 0.1163 acc 0.9600\n",
      "loss 0.0502 acc 0.9800\n",
      "loss 0.0378 acc 0.9900\n",
      "Current iteration 49\n",
      "loss 0.0876 acc 0.9800\n",
      "loss 0.0847 acc 0.9800\n",
      "loss 0.1001 acc 0.9700\n",
      "loss 0.1163 acc 0.9600\n",
      "loss 0.0493 acc 0.9800\n",
      "loss 0.0371 acc 0.9900\n",
      "Current iteration 50\n",
      "loss 0.0866 acc 0.9800\n",
      "loss 0.0825 acc 0.9800\n",
      "loss 0.0986 acc 0.9700\n",
      "loss 0.1161 acc 0.9600\n",
      "loss 0.0483 acc 0.9800\n",
      "loss 0.0364 acc 1.0000\n",
      "Current iteration 51\n",
      "loss 0.0856 acc 0.9800\n",
      "loss 0.0804 acc 0.9800\n",
      "loss 0.0972 acc 0.9700\n",
      "loss 0.1161 acc 0.9600\n",
      "loss 0.0474 acc 0.9800\n",
      "loss 0.0356 acc 1.0000\n",
      "Current iteration 52\n",
      "loss 0.0848 acc 0.9800\n",
      "loss 0.0786 acc 0.9800\n",
      "loss 0.0958 acc 0.9700\n",
      "loss 0.1163 acc 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0466 acc 0.9800\n",
      "loss 0.0351 acc 1.0000\n",
      "Current iteration 53\n",
      "loss 0.0837 acc 0.9800\n",
      "loss 0.0769 acc 0.9800\n",
      "loss 0.0946 acc 0.9700\n",
      "loss 0.1164 acc 0.9600\n",
      "loss 0.0456 acc 0.9800\n",
      "loss 0.0344 acc 1.0000\n",
      "Current iteration 54\n",
      "loss 0.0828 acc 0.9800\n",
      "loss 0.0751 acc 0.9800\n",
      "loss 0.0933 acc 0.9700\n",
      "loss 0.1166 acc 0.9600\n",
      "loss 0.0447 acc 0.9800\n",
      "loss 0.0337 acc 1.0000\n",
      "Current iteration 55\n",
      "loss 0.0819 acc 0.9800\n",
      "loss 0.0736 acc 0.9800\n",
      "loss 0.0921 acc 0.9800\n",
      "loss 0.1169 acc 0.9600\n",
      "loss 0.0440 acc 0.9800\n",
      "loss 0.0333 acc 1.0000\n",
      "Current iteration 56\n",
      "loss 0.0810 acc 0.9800\n",
      "loss 0.0721 acc 0.9800\n",
      "loss 0.0909 acc 0.9800\n",
      "loss 0.1169 acc 0.9600\n",
      "loss 0.0432 acc 0.9800\n",
      "loss 0.0328 acc 1.0000\n",
      "Current iteration 57\n",
      "loss 0.0802 acc 0.9800\n",
      "loss 0.0705 acc 0.9800\n",
      "loss 0.0898 acc 0.9800\n",
      "loss 0.1170 acc 0.9600\n",
      "loss 0.0425 acc 0.9800\n",
      "loss 0.0324 acc 1.0000\n",
      "Current iteration 58\n",
      "loss 0.0794 acc 0.9800\n",
      "loss 0.0693 acc 0.9800\n",
      "loss 0.0885 acc 0.9800\n",
      "loss 0.1170 acc 0.9600\n",
      "loss 0.0417 acc 0.9800\n",
      "loss 0.0318 acc 1.0000\n",
      "Current iteration 59\n",
      "loss 0.0785 acc 0.9800\n",
      "loss 0.0680 acc 0.9800\n",
      "loss 0.0876 acc 0.9800\n",
      "loss 0.1170 acc 0.9600\n",
      "loss 0.0409 acc 0.9800\n",
      "loss 0.0312 acc 1.0000\n",
      "Current iteration 60\n",
      "loss 0.0777 acc 0.9800\n",
      "loss 0.0667 acc 0.9800\n",
      "loss 0.0866 acc 0.9900\n",
      "loss 0.1169 acc 0.9600\n",
      "loss 0.0402 acc 0.9800\n",
      "loss 0.0307 acc 1.0000\n",
      "Current iteration 61\n",
      "loss 0.0770 acc 0.9800\n",
      "loss 0.0654 acc 0.9800\n",
      "loss 0.0858 acc 0.9900\n",
      "loss 0.1168 acc 0.9600\n",
      "loss 0.0395 acc 0.9800\n",
      "loss 0.0302 acc 1.0000\n",
      "Current iteration 62\n",
      "loss 0.0762 acc 0.9800\n",
      "loss 0.0643 acc 0.9800\n",
      "loss 0.0848 acc 0.9900\n",
      "loss 0.1168 acc 0.9600\n",
      "loss 0.0387 acc 0.9800\n",
      "loss 0.0298 acc 1.0000\n",
      "Current iteration 63\n",
      "loss 0.0754 acc 0.9800\n",
      "loss 0.0633 acc 0.9800\n",
      "loss 0.0840 acc 0.9900\n",
      "loss 0.1166 acc 0.9600\n",
      "loss 0.0381 acc 0.9800\n",
      "loss 0.0294 acc 1.0000\n",
      "Current iteration 64\n",
      "loss 0.0749 acc 0.9800\n",
      "loss 0.0623 acc 0.9800\n",
      "loss 0.0832 acc 0.9900\n",
      "loss 0.1165 acc 0.9600\n",
      "loss 0.0374 acc 0.9800\n",
      "loss 0.0290 acc 1.0000\n",
      "Current iteration 65\n",
      "loss 0.0743 acc 0.9800\n",
      "loss 0.0614 acc 0.9800\n",
      "loss 0.0825 acc 0.9900\n",
      "loss 0.1165 acc 0.9600\n",
      "loss 0.0367 acc 0.9800\n",
      "loss 0.0286 acc 1.0000\n",
      "Current iteration 66\n",
      "loss 0.0737 acc 0.9800\n",
      "loss 0.0604 acc 0.9800\n",
      "loss 0.0816 acc 0.9900\n",
      "loss 0.1164 acc 0.9600\n",
      "loss 0.0361 acc 0.9800\n",
      "loss 0.0282 acc 1.0000\n",
      "Current iteration 67\n",
      "loss 0.0732 acc 0.9800\n",
      "loss 0.0596 acc 0.9800\n",
      "loss 0.0809 acc 0.9900\n",
      "loss 0.1163 acc 0.9600\n",
      "loss 0.0355 acc 0.9800\n",
      "loss 0.0279 acc 1.0000\n",
      "Current iteration 68\n",
      "loss 0.0726 acc 0.9800\n",
      "loss 0.0585 acc 0.9900\n",
      "loss 0.0801 acc 0.9900\n",
      "loss 0.1162 acc 0.9600\n",
      "loss 0.0348 acc 0.9800\n",
      "loss 0.0276 acc 1.0000\n",
      "Current iteration 69\n",
      "loss 0.0721 acc 0.9800\n",
      "loss 0.0575 acc 0.9900\n",
      "loss 0.0793 acc 0.9900\n",
      "loss 0.1160 acc 0.9600\n",
      "loss 0.0342 acc 0.9800\n",
      "loss 0.0273 acc 1.0000\n",
      "Current iteration 70\n",
      "loss 0.0715 acc 0.9800\n",
      "loss 0.0565 acc 0.9900\n",
      "loss 0.0786 acc 0.9900\n",
      "loss 0.1159 acc 0.9600\n",
      "loss 0.0336 acc 0.9800\n",
      "loss 0.0270 acc 1.0000\n",
      "Current iteration 71\n",
      "loss 0.0709 acc 0.9800\n",
      "loss 0.0556 acc 0.9900\n",
      "loss 0.0778 acc 0.9900\n",
      "loss 0.1159 acc 0.9600\n",
      "loss 0.0330 acc 0.9800\n",
      "loss 0.0268 acc 1.0000\n",
      "Current iteration 72\n",
      "loss 0.0704 acc 0.9800\n",
      "loss 0.0548 acc 0.9900\n",
      "loss 0.0771 acc 0.9900\n",
      "loss 0.1158 acc 0.9600\n",
      "loss 0.0325 acc 0.9800\n",
      "loss 0.0265 acc 1.0000\n",
      "Current iteration 73\n",
      "loss 0.0700 acc 0.9800\n",
      "loss 0.0539 acc 0.9900\n",
      "loss 0.0763 acc 0.9900\n",
      "loss 0.1157 acc 0.9600\n",
      "loss 0.0320 acc 0.9800\n",
      "loss 0.0262 acc 1.0000\n",
      "Current iteration 74\n",
      "loss 0.0695 acc 0.9800\n",
      "loss 0.0532 acc 0.9900\n",
      "loss 0.0756 acc 0.9900\n",
      "loss 0.1155 acc 0.9600\n",
      "loss 0.0315 acc 0.9800\n",
      "loss 0.0260 acc 1.0000\n",
      "Current iteration 75\n",
      "loss 0.0690 acc 0.9800\n",
      "loss 0.0524 acc 0.9900\n",
      "loss 0.0750 acc 0.9900\n",
      "loss 0.1153 acc 0.9600\n",
      "loss 0.0310 acc 0.9800\n",
      "loss 0.0257 acc 1.0000\n",
      "Current iteration 76\n",
      "loss 0.0684 acc 0.9800\n",
      "loss 0.0518 acc 0.9900\n",
      "loss 0.0743 acc 0.9900\n",
      "loss 0.1153 acc 0.9600\n",
      "loss 0.0305 acc 0.9800\n",
      "loss 0.0254 acc 1.0000\n",
      "Current iteration 77\n",
      "loss 0.0680 acc 0.9800\n",
      "loss 0.0511 acc 0.9900\n",
      "loss 0.0736 acc 0.9900\n",
      "loss 0.1152 acc 0.9600\n",
      "loss 0.0301 acc 0.9800\n",
      "loss 0.0252 acc 1.0000\n",
      "Current iteration 78\n",
      "loss 0.0676 acc 0.9800\n",
      "loss 0.0504 acc 0.9900\n",
      "loss 0.0730 acc 0.9900\n",
      "loss 0.1151 acc 0.9600\n",
      "loss 0.0296 acc 0.9800\n",
      "loss 0.0251 acc 1.0000\n",
      "Current iteration 79\n",
      "loss 0.0670 acc 0.9800\n",
      "loss 0.0498 acc 0.9900\n",
      "loss 0.0725 acc 0.9900\n",
      "loss 0.1148 acc 0.9600\n",
      "loss 0.0291 acc 0.9900\n",
      "loss 0.0248 acc 1.0000\n",
      "Current iteration 80\n",
      "loss 0.0667 acc 0.9800\n",
      "loss 0.0492 acc 0.9900\n",
      "loss 0.0720 acc 0.9900\n",
      "loss 0.1143 acc 0.9700\n",
      "loss 0.0287 acc 1.0000\n",
      "loss 0.0246 acc 1.0000\n",
      "Current iteration 81\n",
      "loss 0.0663 acc 0.9800\n",
      "loss 0.0486 acc 0.9900\n",
      "loss 0.0715 acc 0.9900\n",
      "loss 0.1140 acc 0.9700\n",
      "loss 0.0284 acc 1.0000\n",
      "loss 0.0244 acc 1.0000\n",
      "Current iteration 82\n",
      "loss 0.0657 acc 0.9800\n",
      "loss 0.0481 acc 0.9900\n",
      "loss 0.0709 acc 0.9900\n",
      "loss 0.1138 acc 0.9700\n",
      "loss 0.0280 acc 1.0000\n",
      "loss 0.0243 acc 1.0000\n",
      "Current iteration 83\n",
      "loss 0.0653 acc 0.9900\n",
      "loss 0.0475 acc 0.9900\n",
      "loss 0.0705 acc 0.9900\n",
      "loss 0.1137 acc 0.9700\n",
      "loss 0.0277 acc 1.0000\n",
      "loss 0.0241 acc 1.0000\n",
      "Current iteration 84\n",
      "loss 0.0649 acc 0.9900\n",
      "loss 0.0469 acc 0.9900\n",
      "loss 0.0701 acc 0.9900\n",
      "loss 0.1136 acc 0.9700\n",
      "loss 0.0273 acc 1.0000\n",
      "loss 0.0238 acc 1.0000\n",
      "Current iteration 85\n",
      "loss 0.0644 acc 0.9900\n",
      "loss 0.0464 acc 0.9900\n",
      "loss 0.0695 acc 0.9900\n",
      "loss 0.1135 acc 0.9700\n",
      "loss 0.0270 acc 1.0000\n",
      "loss 0.0236 acc 1.0000\n",
      "Current iteration 86\n",
      "loss 0.0640 acc 0.9900\n",
      "loss 0.0459 acc 0.9900\n",
      "loss 0.0690 acc 0.9900\n",
      "loss 0.1133 acc 0.9700\n",
      "loss 0.0267 acc 1.0000\n",
      "loss 0.0234 acc 1.0000\n",
      "Current iteration 87\n",
      "loss 0.0636 acc 0.9900\n",
      "loss 0.0457 acc 0.9900\n",
      "loss 0.0683 acc 0.9900\n",
      "loss 0.1133 acc 0.9700\n",
      "loss 0.0264 acc 1.0000\n",
      "loss 0.0232 acc 1.0000\n",
      "Current iteration 88\n",
      "loss 0.0633 acc 0.9900\n",
      "loss 0.0452 acc 0.9900\n",
      "loss 0.0678 acc 0.9900\n",
      "loss 0.1129 acc 0.9700\n",
      "loss 0.0260 acc 1.0000\n",
      "loss 0.0230 acc 1.0000\n",
      "Current iteration 89\n",
      "loss 0.0629 acc 0.9900\n",
      "loss 0.0447 acc 0.9900\n",
      "loss 0.0671 acc 0.9900\n",
      "loss 0.1128 acc 0.9700\n",
      "loss 0.0257 acc 1.0000\n",
      "loss 0.0229 acc 0.9900\n",
      "Current iteration 90\n",
      "loss 0.0626 acc 0.9900\n",
      "loss 0.0443 acc 0.9900\n",
      "loss 0.0666 acc 0.9900\n",
      "loss 0.1121 acc 0.9700\n",
      "loss 0.0253 acc 1.0000\n",
      "loss 0.0228 acc 0.9900\n",
      "Current iteration 91\n",
      "loss 0.0622 acc 0.9900\n",
      "loss 0.0438 acc 0.9900\n",
      "loss 0.0662 acc 0.9900\n",
      "loss 0.1117 acc 0.9700\n",
      "loss 0.0250 acc 1.0000\n",
      "loss 0.0226 acc 0.9900\n",
      "Current iteration 92\n",
      "loss 0.0619 acc 0.9900\n",
      "loss 0.0434 acc 0.9900\n",
      "loss 0.0656 acc 0.9900\n",
      "loss 0.1113 acc 0.9700\n",
      "loss 0.0247 acc 1.0000\n",
      "loss 0.0225 acc 0.9900\n",
      "Current iteration 93\n",
      "loss 0.0615 acc 0.9900\n",
      "loss 0.0430 acc 0.9900\n",
      "loss 0.0652 acc 0.9900\n",
      "loss 0.1110 acc 0.9700\n",
      "loss 0.0245 acc 1.0000\n",
      "loss 0.0224 acc 0.9900\n",
      "Current iteration 94\n",
      "loss 0.0610 acc 0.9900\n",
      "loss 0.0426 acc 0.9900\n",
      "loss 0.0646 acc 0.9900\n",
      "loss 0.1107 acc 0.9700\n",
      "loss 0.0242 acc 1.0000\n",
      "loss 0.0222 acc 0.9900\n",
      "Current iteration 95\n",
      "loss 0.0607 acc 0.9900\n",
      "loss 0.0422 acc 0.9900\n",
      "loss 0.0641 acc 0.9900\n",
      "loss 0.1104 acc 0.9700\n",
      "loss 0.0239 acc 1.0000\n",
      "loss 0.0221 acc 0.9900\n",
      "Current iteration 96\n",
      "loss 0.0604 acc 0.9900\n",
      "loss 0.0419 acc 0.9900\n",
      "loss 0.0637 acc 0.9900\n",
      "loss 0.1097 acc 0.9700\n",
      "loss 0.0236 acc 1.0000\n",
      "loss 0.0219 acc 0.9900\n",
      "Current iteration 97\n",
      "loss 0.0601 acc 0.9900\n",
      "loss 0.0417 acc 0.9900\n",
      "loss 0.0633 acc 0.9900\n",
      "loss 0.1089 acc 0.9700\n",
      "loss 0.0233 acc 1.0000\n",
      "loss 0.0218 acc 0.9900\n",
      "Current iteration 98\n",
      "loss 0.0598 acc 0.9900\n",
      "loss 0.0413 acc 0.9900\n",
      "loss 0.0629 acc 0.9900\n",
      "loss 0.1085 acc 0.9700\n",
      "loss 0.0230 acc 1.0000\n",
      "loss 0.0217 acc 0.9900\n",
      "Current iteration 99\n",
      "loss 0.0594 acc 0.9900\n",
      "loss 0.0411 acc 0.9900\n",
      "loss 0.0624 acc 0.9900\n",
      "loss 0.1079 acc 0.9700\n",
      "loss 0.0226 acc 1.0000\n",
      "loss 0.0217 acc 0.9900\n",
      "Current iteration 100\n",
      "loss 0.0591 acc 0.9900\n",
      "loss 0.0409 acc 0.9900\n",
      "loss 0.0620 acc 0.9900\n",
      "loss 0.1074 acc 0.9700\n",
      "loss 0.0224 acc 1.0000\n",
      "loss 0.0216 acc 0.9900\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "        for ind_ in range(0, int(60000 / batch_size)):\n",
    "            batch_X = train_data[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = train_label[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            _, cur_loss, cur_acc = sess.run(\n",
    "                [model['opt'], model['loss'], model['acc']],\n",
    "                feed_dict={X: batch_X, by: batch_by})\n",
    "            if ind_ % num_display == 0:\n",
    "                print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    model['saver'].save(sess, './ours.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ours.ckpt\n",
      "TEST: loss 2.3139 acc 0.0826\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    model['saver'].restore(sess,'./ours.ckpt')\n",
    "    \n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    \n",
    "    for ind_ in range(0, 10):\n",
    "        cur_loss, cur_acc = sess.run(\n",
    "                                    [model['loss'], model['acc']],\n",
    "                                    feed_dict={X: test_data[ind_*1000:(ind_+1)*1000],\n",
    "                                              by: test_label[ind_*1000:(ind_+1)*1000]})\n",
    "        cur_loss_all += cur_loss\n",
    "        cur_acc_all += cur_acc\n",
    "    print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all /10.0,\n",
    "                                                 cur_acc_all / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

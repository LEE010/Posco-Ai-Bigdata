{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = unpickle(\"/home/pirl/cifar-100-python/test\")\n",
    "train = unpickle(\"/home/pirl/cifar-100-python/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[b'data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.transpose(train[b'data'].reshape(-1,3,32,32),(0,2,3,1))\n",
    "train_y = train[b'fine_labels']\n",
    "test_x = np.transpose(test[b'data'].reshape(-1,3,32,32),(0,2,3,1))\n",
    "test_y = test[b'fine_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "num_display = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![screensh](https://cdn-images-1.medium.com/max/1600/1*M5NIelQC33eN6KjwZRccoQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(X,filters,name,strides=(1,1)):\n",
    "    outs = tf.layers.conv2d(X, filters, 3, padding='same', name=name, reuse=tf.AUTO_REUSE)\n",
    "    outs = bn(outs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(X):\n",
    "    with tf.variable_scope(name):\n",
    "        batch_mean, batch_var = tf.nn.moments(X,[0])\n",
    "    return tf.nn.batch_normalization(X,batch_mean,batch_var,0,1,1e-3,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X,filters,name):\n",
    "    with tf.variable_scope(name):\n",
    "        outs = conv(X,filters,name)\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = conv(outs,filters,name)\n",
    "        outs = tf.nn.relu(X + outs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def identity_block(X,filters,name):\n",
    "#     with tf.variable_scope(name):\n",
    "#         outs = tf.layers.conv2d(X, filters, 3,(2,2),padding='same', name=name, reuse=tf.AUTO_REUSE)\n",
    "#         outs = tf.layers.conv2d(outs, filters*2,3,padding='same', name=name, reuse=tf.AUTO_REUSE)\n",
    "#     return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_connection(X,filters,name):\n",
    "    with tf.variable_scope(name):\n",
    "        sc = conv(X,filters,name,strides=(2,2))\n",
    "        sc = conv(sc,filters*2,name)\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-b033c19cb839>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-b033c19cb839>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    outs =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def ResNet(X,filter_list,filter_iters):\n",
    "    outs = tf.layers.conv2d(X,filter_list[0],1,padding='same',name=first, reuse=False)\n",
    "    for filters in filter_list:\n",
    "        for i in range(filter_iters[0]):\n",
    "            outs = convolutional_block(outs,filters,'conv-{0}-{1}'.format(filters,i))\n",
    "        if filters == filter_list[-1]:\n",
    "            outs = tf.nn.avg_pool(outs,)\n",
    "        else:\n",
    "            outs = skip_connection(outs)\n",
    "    \n",
    "    outs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8aec1e106a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       _scope=name)\n\u001b[0;32m--> 417\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1461\u001b[0m           \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m           spec.max_ndim is not None):\n\u001b[0;32m-> 1463\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m   1465\u001b[0m                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndims'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope('first'):\n",
    "        tf.get_variable(tf.layers.conv2d(train_x,32,1,padding='same',name='conv'))\n",
    "        sess.run(['conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X, by, is_reuse):\n",
    "#     X = tf.expand_dims(X, axis=3) # (None, 32, 32, 3)\n",
    "    \n",
    "    with tf.variable_scope('first'):\n",
    "        outs = tf.layers.conv2d(X, 128, 3, padding='same', name='conv1', reuse=tf.AUTO_REUSE) # (None, 32, 32, 128)\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs, 2, 2) # (None, 14, 14, 128)\n",
    "    with tf.variable_scope('second'):\n",
    "        outs = tf.layers.conv2d(outs, 256, 3, padding='same', name='conv2')\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs, 2, 2) # (None, 7, 7, 256)\n",
    "    with tf.variable_scope('third'):\n",
    "        outs = tf.layers.conv2d(outs, 64, 3, padding='same', name='conv3')\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.max_pooling2d(outs, 2, 2) # (None, 3, 3, 64)\n",
    "    \n",
    "    outs = tf.reshape(outs, (-1, outs.shape[1]*outs.shape[2]*outs.shape[3]))\n",
    "    \n",
    "    with tf.variable_scope('dense'):\n",
    "        outs = tf.layers.dense(outs,128, name='dense1')\n",
    "        outs = tf.nn.relu(outs)\n",
    "        outs = tf.layers.dense(outs, 100, name='dense2')\n",
    "    \n",
    "    one_hot = tf.one_hot(by, 100)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outs, \n",
    "                                                      labels=one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    preds = tf.cast(tf.argmax(tf.nn.softmax(outs), axis=1), tf.int32)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(by, preds), tf.float32))\n",
    "    saver = tf.train.Saver(\n",
    "            tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,'second')\n",
    "            +tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,'thrid')\n",
    "            +tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,'dense')\n",
    "    )\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'opt': opt,\n",
    "        'preds': preds,\n",
    "        'acc': acc,\n",
    "        'init': init,\n",
    "        'saver': saver\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "by = tf.placeholder(tf.int32)\n",
    "\n",
    "model = get_model(X, by, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 39.4301 acc 0.0100\n",
      "loss 4.6125 acc 0.0100\n",
      "loss 4.5674 acc 0.0200\n",
      "loss 4.5118 acc 0.0500\n",
      "loss 4.5217 acc 0.0300\n",
      "Current iteration 2\n",
      "loss 4.4347 acc 0.0500\n",
      "loss 4.5030 acc 0.0300\n",
      "loss 4.4809 acc 0.0400\n",
      "loss 4.4360 acc 0.0300\n",
      "loss 4.4270 acc 0.0300\n",
      "Current iteration 3\n",
      "loss 4.3155 acc 0.0800\n",
      "loss 4.4205 acc 0.0300\n",
      "loss 4.3932 acc 0.0300\n",
      "loss 4.3387 acc 0.0300\n",
      "loss 4.2994 acc 0.0700\n",
      "Current iteration 4\n",
      "loss 4.2273 acc 0.0700\n",
      "loss 4.3434 acc 0.0300\n",
      "loss 4.3079 acc 0.0500\n",
      "loss 4.2153 acc 0.0500\n",
      "loss 4.1431 acc 0.1100\n",
      "Current iteration 5\n",
      "loss 4.1119 acc 0.0900\n",
      "loss 4.2658 acc 0.0400\n",
      "loss 4.1939 acc 0.1000\n",
      "loss 4.1040 acc 0.0500\n",
      "loss 4.0110 acc 0.1100\n",
      "Current iteration 6\n",
      "loss 3.9600 acc 0.1400\n",
      "loss 4.1927 acc 0.0500\n",
      "loss 4.0645 acc 0.1300\n",
      "loss 3.9704 acc 0.0500\n",
      "loss 3.8890 acc 0.1500\n",
      "Current iteration 7\n",
      "loss 3.8225 acc 0.1800\n",
      "loss 4.1359 acc 0.0600\n",
      "loss 3.9807 acc 0.1400\n",
      "loss 3.8796 acc 0.0800\n",
      "loss 3.7667 acc 0.1500\n",
      "Current iteration 8\n",
      "loss 3.7393 acc 0.2000\n",
      "loss 4.0377 acc 0.0500\n",
      "loss 3.8688 acc 0.1700\n",
      "loss 3.7730 acc 0.1000\n",
      "loss 3.6929 acc 0.1600\n",
      "Current iteration 9\n",
      "loss 3.6761 acc 0.1900\n",
      "loss 3.9610 acc 0.0700\n",
      "loss 3.7856 acc 0.1600\n",
      "loss 3.6714 acc 0.1200\n",
      "loss 3.5977 acc 0.2100\n",
      "Current iteration 10\n",
      "loss 3.5698 acc 0.1500\n",
      "loss 3.9439 acc 0.0700\n",
      "loss 3.6649 acc 0.1700\n",
      "loss 3.5863 acc 0.1300\n",
      "loss 3.4919 acc 0.2300\n",
      "Current iteration 11\n",
      "loss 3.4975 acc 0.1500\n",
      "loss 3.8593 acc 0.0900\n",
      "loss 3.5567 acc 0.1600\n",
      "loss 3.5262 acc 0.1500\n",
      "loss 3.4325 acc 0.2300\n",
      "Current iteration 12\n",
      "loss 3.4258 acc 0.2200\n",
      "loss 3.7821 acc 0.0900\n",
      "loss 3.4673 acc 0.2100\n",
      "loss 3.4587 acc 0.1500\n",
      "loss 3.3318 acc 0.2200\n",
      "Current iteration 13\n",
      "loss 3.3641 acc 0.2200\n",
      "loss 3.7155 acc 0.1100\n",
      "loss 3.3926 acc 0.2000\n",
      "loss 3.3828 acc 0.2100\n",
      "loss 3.2433 acc 0.2300\n",
      "Current iteration 14\n",
      "loss 3.3143 acc 0.2300\n",
      "loss 3.6470 acc 0.1100\n",
      "loss 3.3210 acc 0.2200\n",
      "loss 3.3296 acc 0.2300\n",
      "loss 3.1992 acc 0.2300\n",
      "Current iteration 15\n",
      "loss 3.2560 acc 0.2600\n",
      "loss 3.5679 acc 0.1300\n",
      "loss 3.2502 acc 0.2200\n",
      "loss 3.2924 acc 0.2600\n",
      "loss 3.1393 acc 0.2300\n",
      "Current iteration 16\n",
      "loss 3.2076 acc 0.2500\n",
      "loss 3.4949 acc 0.1500\n",
      "loss 3.1733 acc 0.2400\n",
      "loss 3.2265 acc 0.2300\n",
      "loss 3.0815 acc 0.2200\n",
      "Current iteration 17\n",
      "loss 3.1860 acc 0.2600\n",
      "loss 3.4256 acc 0.1600\n",
      "loss 3.1317 acc 0.2300\n",
      "loss 3.1703 acc 0.2600\n",
      "loss 3.0334 acc 0.2500\n",
      "Current iteration 18\n",
      "loss 3.1443 acc 0.2700\n",
      "loss 3.3449 acc 0.1600\n",
      "loss 3.0833 acc 0.2600\n",
      "loss 3.1031 acc 0.2700\n",
      "loss 2.9894 acc 0.2400\n",
      "Current iteration 19\n",
      "loss 3.1041 acc 0.2700\n",
      "loss 3.2709 acc 0.1800\n",
      "loss 3.0225 acc 0.2500\n",
      "loss 3.0572 acc 0.2800\n",
      "loss 2.9577 acc 0.2500\n",
      "Current iteration 20\n",
      "loss 3.0774 acc 0.2800\n",
      "loss 3.2330 acc 0.1700\n",
      "loss 3.0055 acc 0.2300\n",
      "loss 3.0010 acc 0.3100\n",
      "loss 2.9358 acc 0.2500\n",
      "Current iteration 21\n",
      "loss 3.0420 acc 0.2900\n",
      "loss 3.1530 acc 0.1900\n",
      "loss 2.9372 acc 0.2400\n",
      "loss 2.9429 acc 0.3200\n",
      "loss 2.8898 acc 0.2700\n",
      "Current iteration 22\n",
      "loss 2.9921 acc 0.2900\n",
      "loss 3.0902 acc 0.2200\n",
      "loss 2.9038 acc 0.2600\n",
      "loss 2.9198 acc 0.3100\n",
      "loss 2.8595 acc 0.2600\n",
      "Current iteration 23\n",
      "loss 2.9552 acc 0.3200\n",
      "loss 3.0433 acc 0.2100\n",
      "loss 2.8572 acc 0.2800\n",
      "loss 2.8630 acc 0.3200\n",
      "loss 2.8240 acc 0.2700\n",
      "Current iteration 24\n",
      "loss 2.9277 acc 0.3400\n",
      "loss 2.9691 acc 0.2400\n",
      "loss 2.8070 acc 0.3000\n",
      "loss 2.8155 acc 0.3700\n",
      "loss 2.8087 acc 0.2700\n",
      "Current iteration 25\n",
      "loss 2.9093 acc 0.3400\n",
      "loss 2.9080 acc 0.2800\n",
      "loss 2.7802 acc 0.3000\n",
      "loss 2.7866 acc 0.3800\n",
      "loss 2.7823 acc 0.2800\n",
      "Current iteration 26\n",
      "loss 2.8761 acc 0.3300\n",
      "loss 2.8573 acc 0.2800\n",
      "loss 2.7445 acc 0.3000\n",
      "loss 2.7413 acc 0.3600\n",
      "loss 2.7750 acc 0.2700\n",
      "Current iteration 27\n",
      "loss 2.8386 acc 0.3500\n",
      "loss 2.8202 acc 0.2900\n",
      "loss 2.7099 acc 0.3000\n",
      "loss 2.6816 acc 0.3900\n",
      "loss 2.7653 acc 0.2900\n",
      "Current iteration 28\n",
      "loss 2.8148 acc 0.3300\n",
      "loss 2.7733 acc 0.3100\n",
      "loss 2.6617 acc 0.3100\n",
      "loss 2.6402 acc 0.3700\n",
      "loss 2.7529 acc 0.2800\n",
      "Current iteration 29\n",
      "loss 2.7770 acc 0.3200\n",
      "loss 2.7280 acc 0.3100\n",
      "loss 2.6297 acc 0.3100\n",
      "loss 2.5998 acc 0.4200\n",
      "loss 2.7483 acc 0.2600\n",
      "Current iteration 30\n",
      "loss 2.7464 acc 0.3400\n",
      "loss 2.6787 acc 0.3200\n",
      "loss 2.5909 acc 0.3400\n",
      "loss 2.5668 acc 0.4000\n",
      "loss 2.7143 acc 0.2600\n",
      "Current iteration 31\n",
      "loss 2.7138 acc 0.3300\n",
      "loss 2.6438 acc 0.3500\n",
      "loss 2.5705 acc 0.3400\n",
      "loss 2.5250 acc 0.4100\n",
      "loss 2.7057 acc 0.2700\n",
      "Current iteration 32\n",
      "loss 2.6638 acc 0.3600\n",
      "loss 2.6325 acc 0.3400\n",
      "loss 2.5386 acc 0.3700\n",
      "loss 2.5055 acc 0.4100\n",
      "loss 2.6614 acc 0.2900\n",
      "Current iteration 33\n",
      "loss 2.6318 acc 0.3600\n",
      "loss 2.5929 acc 0.3400\n",
      "loss 2.4831 acc 0.3600\n",
      "loss 2.4753 acc 0.4100\n",
      "loss 2.6415 acc 0.3000\n",
      "Current iteration 34\n",
      "loss 2.5971 acc 0.3600\n",
      "loss 2.5627 acc 0.3200\n",
      "loss 2.4704 acc 0.3600\n",
      "loss 2.4411 acc 0.4000\n",
      "loss 2.6220 acc 0.3000\n",
      "Current iteration 35\n",
      "loss 2.5624 acc 0.3700\n",
      "loss 2.5318 acc 0.3500\n",
      "loss 2.4373 acc 0.3700\n",
      "loss 2.4255 acc 0.4500\n",
      "loss 2.5895 acc 0.3000\n",
      "Current iteration 36\n",
      "loss 2.5162 acc 0.3900\n",
      "loss 2.5102 acc 0.3500\n",
      "loss 2.3812 acc 0.3800\n",
      "loss 2.3993 acc 0.4500\n",
      "loss 2.5809 acc 0.2800\n",
      "Current iteration 37\n",
      "loss 2.5022 acc 0.3900\n",
      "loss 2.4888 acc 0.3600\n",
      "loss 2.3429 acc 0.3600\n",
      "loss 2.3704 acc 0.4700\n",
      "loss 2.5681 acc 0.2800\n",
      "Current iteration 38\n",
      "loss 2.4685 acc 0.3800\n",
      "loss 2.4475 acc 0.3600\n",
      "loss 2.2920 acc 0.3800\n",
      "loss 2.3483 acc 0.4700\n",
      "loss 2.5790 acc 0.3000\n",
      "Current iteration 39\n",
      "loss 2.4291 acc 0.3800\n",
      "loss 2.4318 acc 0.3600\n",
      "loss 2.2505 acc 0.4000\n",
      "loss 2.3453 acc 0.4800\n",
      "loss 2.5932 acc 0.3000\n",
      "Current iteration 40\n",
      "loss 2.3854 acc 0.3900\n",
      "loss 2.4055 acc 0.3600\n",
      "loss 2.2122 acc 0.4200\n",
      "loss 2.2971 acc 0.4900\n",
      "loss 2.5324 acc 0.3100\n",
      "Current iteration 41\n",
      "loss 2.3599 acc 0.3900\n",
      "loss 2.3833 acc 0.3900\n",
      "loss 2.1854 acc 0.4100\n",
      "loss 2.2810 acc 0.4800\n",
      "loss 2.5218 acc 0.3200\n",
      "Current iteration 42\n",
      "loss 2.3348 acc 0.4000\n",
      "loss 2.3487 acc 0.4000\n",
      "loss 2.1337 acc 0.4000\n",
      "loss 2.2336 acc 0.5000\n",
      "loss 2.4896 acc 0.3200\n",
      "Current iteration 43\n",
      "loss 2.3109 acc 0.4200\n",
      "loss 2.3049 acc 0.4200\n",
      "loss 2.1049 acc 0.4200\n",
      "loss 2.2019 acc 0.5200\n",
      "loss 2.4733 acc 0.3000\n",
      "Current iteration 44\n",
      "loss 2.2660 acc 0.4200\n",
      "loss 2.2876 acc 0.4100\n",
      "loss 2.0789 acc 0.4400\n",
      "loss 2.1781 acc 0.5100\n",
      "loss 2.4692 acc 0.3400\n",
      "Current iteration 45\n",
      "loss 2.2496 acc 0.4400\n",
      "loss 2.2473 acc 0.4200\n",
      "loss 2.0283 acc 0.4400\n",
      "loss 2.1427 acc 0.5500\n",
      "loss 2.4629 acc 0.3400\n",
      "Current iteration 46\n",
      "loss 2.2209 acc 0.4500\n",
      "loss 2.2155 acc 0.4400\n",
      "loss 2.0068 acc 0.4600\n",
      "loss 2.1063 acc 0.5700\n",
      "loss 2.3961 acc 0.3400\n",
      "Current iteration 47\n",
      "loss 2.1967 acc 0.4600\n",
      "loss 2.1817 acc 0.4700\n",
      "loss 1.9627 acc 0.4600\n",
      "loss 2.0746 acc 0.5400\n",
      "loss 2.4002 acc 0.3500\n",
      "Current iteration 48\n",
      "loss 2.1751 acc 0.4700\n",
      "loss 2.1482 acc 0.4400\n",
      "loss 1.9391 acc 0.4600\n",
      "loss 2.0470 acc 0.5800\n",
      "loss 2.3776 acc 0.3900\n",
      "Current iteration 49\n",
      "loss 2.1565 acc 0.4500\n",
      "loss 2.1177 acc 0.4500\n",
      "loss 1.9243 acc 0.4600\n",
      "loss 2.0364 acc 0.5600\n",
      "loss 2.3563 acc 0.3700\n",
      "Current iteration 50\n",
      "loss 2.1257 acc 0.4500\n",
      "loss 2.0925 acc 0.4700\n",
      "loss 1.9175 acc 0.4600\n",
      "loss 2.0025 acc 0.5600\n",
      "loss 2.3331 acc 0.3900\n",
      "Current iteration 51\n",
      "loss 2.1027 acc 0.4600\n",
      "loss 2.0690 acc 0.4500\n",
      "loss 1.8311 acc 0.5100\n",
      "loss 1.9581 acc 0.6100\n",
      "loss 2.3020 acc 0.3800\n",
      "Current iteration 52\n",
      "loss 2.0767 acc 0.4700\n",
      "loss 2.0386 acc 0.4800\n",
      "loss 1.8094 acc 0.5300\n",
      "loss 1.9725 acc 0.5800\n",
      "loss 2.2969 acc 0.3800\n",
      "Current iteration 53\n",
      "loss 2.0557 acc 0.4800\n",
      "loss 2.0157 acc 0.4700\n",
      "loss 1.7750 acc 0.5600\n",
      "loss 1.9255 acc 0.6000\n",
      "loss 2.2637 acc 0.3700\n",
      "Current iteration 54\n",
      "loss 2.0382 acc 0.4800\n",
      "loss 1.9686 acc 0.4800\n",
      "loss 1.7805 acc 0.5500\n",
      "loss 1.8937 acc 0.6100\n",
      "loss 2.2406 acc 0.3900\n",
      "Current iteration 55\n",
      "loss 2.0012 acc 0.4800\n",
      "loss 1.9435 acc 0.4800\n",
      "loss 1.7124 acc 0.5700\n",
      "loss 1.8889 acc 0.6000\n",
      "loss 2.2018 acc 0.4100\n",
      "Current iteration 56\n",
      "loss 1.9697 acc 0.4900\n",
      "loss 1.9231 acc 0.4600\n",
      "loss 1.6903 acc 0.5700\n",
      "loss 1.8390 acc 0.6100\n",
      "loss 2.2035 acc 0.4100\n",
      "Current iteration 57\n",
      "loss 1.9456 acc 0.4700\n",
      "loss 1.8988 acc 0.4900\n",
      "loss 1.6445 acc 0.5900\n",
      "loss 1.8167 acc 0.6400\n",
      "loss 2.1817 acc 0.3900\n",
      "Current iteration 58\n",
      "loss 1.9048 acc 0.4900\n",
      "loss 1.8866 acc 0.4800\n",
      "loss 1.6316 acc 0.5800\n",
      "loss 1.8116 acc 0.6300\n",
      "loss 2.1348 acc 0.4300\n",
      "Current iteration 59\n",
      "loss 1.8847 acc 0.5000\n",
      "loss 1.8288 acc 0.4600\n",
      "loss 1.6013 acc 0.5700\n",
      "loss 1.8299 acc 0.6000\n",
      "loss 2.1295 acc 0.4200\n",
      "Current iteration 60\n",
      "loss 1.8527 acc 0.5300\n",
      "loss 1.8158 acc 0.4900\n",
      "loss 1.5984 acc 0.6100\n",
      "loss 1.7400 acc 0.6700\n",
      "loss 2.0698 acc 0.4300\n",
      "Current iteration 61\n",
      "loss 1.8189 acc 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.7797 acc 0.5000\n",
      "loss 1.5335 acc 0.6100\n",
      "loss 1.7259 acc 0.6500\n",
      "loss 2.0857 acc 0.4400\n",
      "Current iteration 62\n",
      "loss 1.7969 acc 0.5500\n",
      "loss 1.7598 acc 0.5100\n",
      "loss 1.5213 acc 0.5900\n",
      "loss 1.7253 acc 0.6300\n",
      "loss 2.0472 acc 0.4400\n",
      "Current iteration 63\n",
      "loss 1.7639 acc 0.5600\n",
      "loss 1.7420 acc 0.5400\n",
      "loss 1.4980 acc 0.6200\n",
      "loss 1.6795 acc 0.6600\n",
      "loss 2.0380 acc 0.4500\n",
      "Current iteration 64\n",
      "loss 1.7279 acc 0.5400\n",
      "loss 1.7079 acc 0.5300\n",
      "loss 1.4893 acc 0.5900\n",
      "loss 1.7153 acc 0.6200\n",
      "loss 2.0280 acc 0.4500\n",
      "Current iteration 65\n",
      "loss 1.7008 acc 0.5700\n",
      "loss 1.7068 acc 0.5000\n",
      "loss 1.4625 acc 0.5900\n",
      "loss 1.6560 acc 0.6500\n",
      "loss 1.9665 acc 0.4600\n",
      "Current iteration 66\n",
      "loss 1.6878 acc 0.5500\n",
      "loss 1.6834 acc 0.5400\n",
      "loss 1.4216 acc 0.6200\n",
      "loss 1.6654 acc 0.6400\n",
      "loss 1.9621 acc 0.4800\n",
      "Current iteration 67\n",
      "loss 1.6534 acc 0.6000\n",
      "loss 1.6833 acc 0.5700\n",
      "loss 1.4021 acc 0.6100\n",
      "loss 1.6106 acc 0.6600\n",
      "loss 1.9211 acc 0.4600\n",
      "Current iteration 68\n",
      "loss 1.6243 acc 0.5800\n",
      "loss 1.6326 acc 0.5700\n",
      "loss 1.3776 acc 0.6100\n",
      "loss 1.6184 acc 0.6500\n",
      "loss 1.9204 acc 0.4600\n",
      "Current iteration 69\n",
      "loss 1.6118 acc 0.6100\n",
      "loss 1.6156 acc 0.5800\n",
      "loss 1.3640 acc 0.6100\n",
      "loss 1.5865 acc 0.6700\n",
      "loss 1.8770 acc 0.4700\n",
      "Current iteration 70\n",
      "loss 1.5756 acc 0.5900\n",
      "loss 1.6121 acc 0.5800\n",
      "loss 1.3455 acc 0.6300\n",
      "loss 1.5452 acc 0.6900\n",
      "loss 1.8690 acc 0.4600\n",
      "Current iteration 71\n",
      "loss 1.5481 acc 0.6100\n",
      "loss 1.5726 acc 0.6000\n",
      "loss 1.3392 acc 0.6300\n",
      "loss 1.5346 acc 0.6700\n",
      "loss 1.8450 acc 0.5100\n",
      "Current iteration 72\n",
      "loss 1.5193 acc 0.5900\n",
      "loss 1.5498 acc 0.5600\n",
      "loss 1.3078 acc 0.6300\n",
      "loss 1.5157 acc 0.6800\n",
      "loss 1.8166 acc 0.5000\n",
      "Current iteration 73\n",
      "loss 1.4726 acc 0.6300\n",
      "loss 1.5295 acc 0.5900\n",
      "loss 1.2852 acc 0.6500\n",
      "loss 1.5016 acc 0.7000\n",
      "loss 1.7661 acc 0.5400\n",
      "Current iteration 74\n",
      "loss 1.4736 acc 0.6100\n",
      "loss 1.5474 acc 0.5900\n",
      "loss 1.2927 acc 0.6600\n",
      "loss 1.4785 acc 0.6700\n",
      "loss 1.7767 acc 0.5300\n",
      "Current iteration 75\n",
      "loss 1.4273 acc 0.6400\n",
      "loss 1.4885 acc 0.6200\n",
      "loss 1.2796 acc 0.6700\n",
      "loss 1.4796 acc 0.6900\n",
      "loss 1.7298 acc 0.5500\n",
      "Current iteration 76\n",
      "loss 1.4118 acc 0.6200\n",
      "loss 1.4638 acc 0.6200\n",
      "loss 1.2271 acc 0.6600\n",
      "loss 1.4346 acc 0.6900\n",
      "loss 1.7538 acc 0.5600\n",
      "Current iteration 77\n",
      "loss 1.3825 acc 0.6300\n",
      "loss 1.4499 acc 0.6100\n",
      "loss 1.2189 acc 0.6800\n",
      "loss 1.4075 acc 0.6900\n",
      "loss 1.6984 acc 0.5300\n",
      "Current iteration 78\n",
      "loss 1.3572 acc 0.6400\n",
      "loss 1.4454 acc 0.6300\n",
      "loss 1.2098 acc 0.6800\n",
      "loss 1.3999 acc 0.6800\n",
      "loss 1.6477 acc 0.5500\n",
      "Current iteration 79\n",
      "loss 1.3218 acc 0.6600\n",
      "loss 1.3851 acc 0.6100\n",
      "loss 1.1922 acc 0.6900\n",
      "loss 1.4236 acc 0.6600\n",
      "loss 1.6561 acc 0.5700\n",
      "Current iteration 80\n",
      "loss 1.3083 acc 0.6700\n",
      "loss 1.3802 acc 0.6300\n",
      "loss 1.1706 acc 0.7000\n",
      "loss 1.3621 acc 0.7000\n",
      "loss 1.6043 acc 0.5900\n",
      "Current iteration 81\n",
      "loss 1.3085 acc 0.6700\n",
      "loss 1.3440 acc 0.6200\n",
      "loss 1.1581 acc 0.6900\n",
      "loss 1.3316 acc 0.6900\n",
      "loss 1.5731 acc 0.6000\n",
      "Current iteration 82\n",
      "loss 1.2825 acc 0.6900\n",
      "loss 1.3219 acc 0.6400\n",
      "loss 1.1252 acc 0.6900\n",
      "loss 1.3275 acc 0.6900\n",
      "loss 1.5680 acc 0.6000\n",
      "Current iteration 83\n",
      "loss 1.2400 acc 0.6900\n",
      "loss 1.2972 acc 0.6400\n",
      "loss 1.1341 acc 0.7100\n",
      "loss 1.2963 acc 0.7000\n",
      "loss 1.5270 acc 0.6200\n",
      "Current iteration 84\n",
      "loss 1.2203 acc 0.6900\n",
      "loss 1.2889 acc 0.6500\n",
      "loss 1.0767 acc 0.7300\n",
      "loss 1.3417 acc 0.6600\n",
      "loss 1.5258 acc 0.6200\n",
      "Current iteration 85\n",
      "loss 1.2038 acc 0.7200\n",
      "loss 1.2875 acc 0.6600\n",
      "loss 1.0523 acc 0.7300\n",
      "loss 1.2398 acc 0.7300\n",
      "loss 1.5063 acc 0.6100\n",
      "Current iteration 86\n",
      "loss 1.1539 acc 0.7400\n",
      "loss 1.2590 acc 0.6300\n",
      "loss 1.1051 acc 0.7100\n",
      "loss 1.2767 acc 0.7100\n",
      "loss 1.4752 acc 0.6000\n",
      "Current iteration 87\n",
      "loss 1.1585 acc 0.7400\n",
      "loss 1.2496 acc 0.6200\n",
      "loss 1.0260 acc 0.7500\n",
      "loss 1.1963 acc 0.7200\n",
      "loss 1.4998 acc 0.5900\n",
      "Current iteration 88\n",
      "loss 1.1381 acc 0.7100\n",
      "loss 1.2337 acc 0.6200\n",
      "loss 0.9940 acc 0.7600\n",
      "loss 1.1971 acc 0.7300\n",
      "loss 1.4419 acc 0.6200\n",
      "Current iteration 89\n",
      "loss 1.0966 acc 0.7500\n",
      "loss 1.2014 acc 0.6500\n",
      "loss 0.9858 acc 0.7500\n",
      "loss 1.1505 acc 0.7200\n",
      "loss 1.4744 acc 0.6300\n",
      "Current iteration 90\n",
      "loss 1.0974 acc 0.7300\n",
      "loss 1.2112 acc 0.6500\n",
      "loss 0.9597 acc 0.7800\n",
      "loss 1.1878 acc 0.7100\n",
      "loss 1.3841 acc 0.6500\n",
      "Current iteration 91\n",
      "loss 1.0529 acc 0.7300\n",
      "loss 1.1915 acc 0.6400\n",
      "loss 0.9608 acc 0.7700\n",
      "loss 1.1162 acc 0.7400\n",
      "loss 1.3633 acc 0.6800\n",
      "Current iteration 92\n",
      "loss 1.0711 acc 0.7000\n",
      "loss 1.1570 acc 0.7100\n",
      "loss 0.9598 acc 0.7500\n",
      "loss 1.1045 acc 0.7300\n",
      "loss 1.3567 acc 0.6800\n",
      "Current iteration 93\n",
      "loss 1.0095 acc 0.7100\n",
      "loss 1.2211 acc 0.6800\n",
      "loss 0.9105 acc 0.7800\n",
      "loss 1.1223 acc 0.7100\n",
      "loss 1.3293 acc 0.6800\n",
      "Current iteration 94\n",
      "loss 0.9936 acc 0.7200\n",
      "loss 1.1289 acc 0.7100\n",
      "loss 0.8845 acc 0.7600\n",
      "loss 1.0460 acc 0.7600\n",
      "loss 1.3375 acc 0.6500\n",
      "Current iteration 95\n",
      "loss 0.9663 acc 0.7400\n",
      "loss 1.0872 acc 0.7100\n",
      "loss 0.8812 acc 0.7800\n",
      "loss 1.0540 acc 0.7300\n",
      "loss 1.2713 acc 0.7000\n",
      "Current iteration 96\n",
      "loss 0.9351 acc 0.7300\n",
      "loss 1.1192 acc 0.6600\n",
      "loss 0.8684 acc 0.7700\n",
      "loss 1.0111 acc 0.7400\n",
      "loss 1.2828 acc 0.7000\n",
      "Current iteration 97\n",
      "loss 0.9341 acc 0.7300\n",
      "loss 1.0515 acc 0.7200\n",
      "loss 0.8705 acc 0.7900\n",
      "loss 1.0202 acc 0.7600\n",
      "loss 1.2555 acc 0.6800\n",
      "Current iteration 98\n",
      "loss 0.9011 acc 0.7500\n",
      "loss 1.0759 acc 0.7100\n",
      "loss 0.8194 acc 0.7900\n",
      "loss 0.9535 acc 0.7800\n",
      "loss 1.2307 acc 0.6900\n",
      "Current iteration 99\n",
      "loss 0.9062 acc 0.7400\n",
      "loss 1.0330 acc 0.7100\n",
      "loss 0.8538 acc 0.7900\n",
      "loss 0.9574 acc 0.7700\n",
      "loss 1.2011 acc 0.6900\n",
      "Current iteration 100\n",
      "loss 0.8553 acc 0.7700\n",
      "loss 1.0802 acc 0.7100\n",
      "loss 0.7951 acc 0.8000\n",
      "loss 0.9160 acc 0.7600\n",
      "loss 1.1762 acc 0.7100\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "        for ind_ in range(0, int(50000 / batch_size)):\n",
    "            batch_X = train_x[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = train_y[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            \n",
    "            _, cur_loss, cur_acc = sess.run([model['opt'], model['loss'], \n",
    "                                             model['acc']],\n",
    "                                             feed_dict={X: batch_X, by: batch_by}\n",
    "                                           )\n",
    "            if ind_ % num_display == 0:\n",
    "                print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    model['saver'].save(sess, './ours.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
